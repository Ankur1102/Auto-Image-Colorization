{"cells":[{"cell_type":"markdown","metadata":{"id":"MfNG_du5R4v9"},"source":["# models"]},{"cell_type":"code","source":["import torch.nn as nn\n","import torchvision.models as models\n","import torch.nn.functional as F\n","\n","\n","class ColorizationNet1(nn.Module):\n","    def __init__(self, input_channels=1):\n","        super(ColorizationNet1, self).__init__()\n","\n","        # ResNet-18\n","        resnet = models.resnet18(num_classes=1000)\n","        resnet.conv1.weight = nn.Parameter(resnet.conv1.weight.sum(dim=1).unsqueeze(1))\n","        self.midlevel_resnet = nn.Sequential(*list(resnet.children())[:6])\n","\n","        # upsampling layers\n","        self.conv1 = nn.Conv2d(128, 64, 3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.conv3 = nn.Conv2d(64, 32, 3, padding=1)\n","        self.bn3 = nn.BatchNorm2d(32)\n","        self.conv4 = nn.Conv2d(32, 16, 3, padding=1)\n","        self.bn4 = nn.BatchNorm2d(16)\n","        self.conv5 = nn.Conv2d(16, 2, 3, padding=1)\n","\n","    def forward(self, input):\n","        # Mid-level features from ResNet\n","        midlevel_features = self.midlevel_resnet(input)\n","\n","        # upsample block\n","        x = F.relu(self.bn1(self.conv1(midlevel_features)))\n","        x = F.interpolate(x, scale_factor=2, mode='nearest')\n","        x = F.relu(self.bn2(self.conv2(x)))\n","        x = F.interpolate(x, scale_factor=2, mode='nearest')\n","        x = F.relu(self.bn3(self.conv3(x)))\n","        x = F.relu(self.bn4(self.conv4(x)))\n","        x = self.conv5(x)\n","        x = F.interpolate(x, scale_factor=2, mode='nearest')\n","\n","        return x\n"],"metadata":{"id":"VaZ99AUfIJPM","executionInfo":{"status":"ok","timestamp":1681988356440,"user_tz":240,"elapsed":6129,"user":{"displayName":"Sonu Kumar","userId":"00783746369091988188"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","import torchvision.models as models\n","import torch.nn.functional as F\n","\n","\n","class ColorizationNet2(nn.Module):\n","    def __init__(self, input_channels=1):\n","        super(ColorizationNet2, self).__init__()\n","\n","        resnet = models.resnet18(num_classes=1000)\n","        resnet.conv1.weight = nn.Parameter(resnet.conv1.weight.sum(dim=1).unsqueeze(1))\n","        self.midlevel_resnet = nn.Sequential(*list(resnet.children())[:6])\n","\n","        self.conv1 = nn.Conv2d(128, 64, 3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.conv3 = nn.Conv2d(64, 32, 3, padding=1)\n","        self.bn3 = nn.BatchNorm2d(32)\n","        self.conv4 = nn.Conv2d(32, 16, 3, padding=1)\n","        self.bn4 = nn.BatchNorm2d(16)\n","        self.conv5 = nn.Conv2d(16, 2, 3, padding=1)\n","\n","        self.upsample1 = nn.Upsample(scale_factor=2)\n","        self.upsample2 = nn.Upsample(scale_factor=2)\n","        self.upsample3 = nn.Upsample(scale_factor=2)\n","\n","    def forward(self, input):\n","        midlevel_features = self.midlevel_resnet(input)\n","\n","        x = F.relu(self.bn1(self.conv1(midlevel_features)))\n","        x = self.upsample1(x)\n","        x = F.relu(self.bn2(self.conv2(x)))\n","        x = self.upsample2(x)\n","        x = F.relu(self.bn3(self.conv3(x)))\n","        x = F.relu(self.bn4(self.conv4(x)))\n","        x = self.conv5(x)\n","        x = self.upsample3(x)\n","\n","        return x\n","\n"],"metadata":{"id":"zxtPJwVjKKQS","executionInfo":{"status":"ok","timestamp":1681985744306,"user_tz":240,"elapsed":492,"user":{"displayName":"Sonu Kumar","userId":"00783746369091988188"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","import torchvision.models as models\n","import torch.nn.functional as F\n","\n","\n","class ColorizationNet3(nn.Module):\n","    def __init__(self, input_channels=1):\n","        super(ColorizationNet3, self).__init__()\n","\n","        resnet = models.resnet18(num_classes=1000)\n","        resnet.conv1.weight = nn.Parameter(resnet.conv1.weight.sum(dim=1).unsqueeze(1))\n","        self.midlevel_resnet = nn.Sequential(*list(resnet.children())[:6])\n","\n","        self.conv1 = nn.Conv2d(128, 128, 3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(128)\n","        self.conv2 = nn.Conv2d(128, 64, 3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n","        self.bn3 = nn.BatchNorm2d(64)\n","        self.conv4 = nn.Conv2d(64, 32, 3, padding=1)\n","        self.bn4 = nn.BatchNorm2d(32)\n","        self.conv5 = nn.Conv2d(32, 32, 3, padding=1)\n","        self.bn5 = nn.BatchNorm2d(32)\n","        self.conv6 = nn.Conv2d(32, 16, 3, padding=1)\n","        self.bn6 = nn.BatchNorm2d(16)\n","        self.conv7 = nn.Conv2d(16, 2, 3, padding=1)\n","\n","        self.upsample1 = nn.Upsample(scale_factor=2)\n","        self.upsample2 = nn.Upsample(scale_factor=2)\n","        self.upsample3 = nn.Upsample(scale_factor=2)\n","\n","    def forward(self, input):\n"," \n","        midlevel_features = self.midlevel_resnet(input)\n","\n","        x = F.relu(self.bn1(self.conv1(midlevel_features)))\n","        x = F.relu(self.bn2(self.conv2(x)))\n","        x = self.upsample1(x)\n","        x = F.relu(self.bn3(self.conv3(x)))\n","        x = self.upsample2(x)\n","        x = F.relu(self.bn4(self.conv4(x)))\n","        x = F.relu(self.bn5(self.conv5(x)))\n","        x = F.relu(self.bn6(self.conv6(x)))\n","        x = self.conv7(x)\n","        x = self.upsample3(x)\n","\n","        return x\n"],"metadata":{"id":"5gbLxYJ_L_VX","executionInfo":{"status":"ok","timestamp":1681986310574,"user_tz":240,"elapsed":425,"user":{"displayName":"Sonu Kumar","userId":"00783746369091988188"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GiqE5mO0xob9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# colorize data "],"metadata":{"id":"tZE_3bzA4zk_"}},{"cell_type":"code","source":["import torchvision.transforms as T\n","import torch\n","import numpy as np\n","from skimage.color import rgb2lab, rgb2gray, lab2rgb\n","from torchvision import datasets\n","from torchvision.datasets.folder import default_loader\n","\n","class ColorizeData(datasets.ImageFolder):\n","\n","    def __init__(self, root, transform=None, target_transform=None, loader=default_loader, is_train=True):\n","        super(ColorizeData, self).__init__(root, transform=transform, target_transform=target_transform, loader=loader)\n","        self.is_train = is_train\n","\n","    def __getitem__(self, index):\n","        path, _ = self.imgs[index]\n","        input = self.loader(path)\n","        input = self.transform(input)\n","        input = np.asarray(input)\n","        input = np.transpose(input, (1, 2, 0))  # Add this line to change the dimensions\n","        img_lab = rgb2lab(input)\n","        img_lab = (img_lab + 128) / 255\n","        img_ab = img_lab[:, :, 1:3]\n","        img_ab = torch.from_numpy(img_ab.transpose((2, 0, 1))).float()\n","        input = rgb2gray(input)\n","        input = torch.from_numpy(input).unsqueeze(0).float()\n","        return input, img_ab\n","\n","\n","def get_colorize_data_transforms(is_train=True):\n","    if is_train:\n","        return T.Compose([\n","            T.RandomResizedCrop(224),\n","            T.RandomHorizontalFlip(),\n","            T.ToTensor(),\n","        ])\n","    else:\n","        return T.Compose([\n","            T.Resize(256),\n","            T.CenterCrop(224),\n","            T.ToTensor(),\n","        ])"],"metadata":{"id":"NNBleOYk2bkt","executionInfo":{"status":"ok","timestamp":1681988361332,"user_tz":240,"elapsed":600,"user":{"displayName":"Sonu Kumar","userId":"00783746369091988188"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# train "],"metadata":{"id":"VTQvU70J4uBs"}},{"cell_type":"code","source":["import torch\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from skimage.color import lab2rgb\n","import time\n","import torch.nn as nn\n","import argparse\n","import torchvision.transforms as T\n","import cv2\n","import glob\n","from torch.utils.tensorboard import SummaryWriter\n","\n","class AverageMeter(object):\n","  def __init__(self):\n","    self.reset()\n","  def reset(self):\n","    self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n","  def update(self, val, n=1):\n","    self.val = val\n","    self.sum += val * n\n","    self.count += n\n","    self.avg = self.sum / self.count\n","\n","class Trainer:\n","    def __init__(self, writer):\n","        self.writer = writer\n","\n","    def to_rgb(self, grayscale_input, ab_input, save_path=None, save_name=None):\n","        # Show/save rgb image from grayscale and ab channels\n","        plt.clf()  # clear matplotlib\n","        color_image = torch.cat((grayscale_input, ab_input), 0).numpy()  # combine channels\n","        color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n","        color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n","        color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128\n","        color_image = lab2rgb(color_image.astype(np.float64))\n","\n","        grayscale_input = grayscale_input.squeeze().numpy()\n","        if save_path is not None and save_name is not None:\n","            plt.imsave(arr=grayscale_input, fname='{}{}'.format(save_path['grayscale'], save_name), cmap='gray')\n","            plt.imsave(arr=color_image, fname='{}{}'.format(save_path['colorized'], save_name))\n","\n","    def train(self, train_loader, epoch, model, criterion, optimizer, scheduler):\n","        print(f'Starting training epoch {epoch + 1}')\n","        model.train()\n","        batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n","        end = time.time()\n","\n","        for i, (input_gray, input_ab) in enumerate(train_loader):\n","\n","            input_gray, input_ab = input_gray.cuda(), input_ab.cuda()\n","            data_time.update(time.time() - end)  \n","\n","            # forward pass\n","            output_ab = model(input_gray)\n","            loss = criterion(output_ab, input_ab)\n","            losses.update(loss.item(), input_gray.size(0))\n","\n","            # gradients and optimize\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            batch_time.update(time.time() - end)\n","            end = time.time()\n","\n","            self.writer.add_scalar(\"Training Loss\", losses.val, epoch * len(train_loader) + i)\n","\n","            if i % 2 == 0:\n","                print(f'Epoch: [{epoch + 1}][{i}/{len(train_loader)}]\\t'\n","                      f'Loss {losses.val:.6f} ({losses.avg:.6f})\\t')\n","\n","        print(f'Finished training epoch {epoch + 1}')\n","\n","    def validate(self, val_loader, epoch, save_images, model, criterion):\n","        model.eval()\n","        batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n","        end = time.time()\n","        already_saved_images = False\n","\n","        for i, (input_gray, input_ab) in enumerate(val_loader):\n","            data_time.update(time.time() - end)\n","            input_gray, input_ab = input_gray.cuda(), input_ab.cuda()\n","\n","            # Run model and record loss\n","            output_ab = model(input_gray)\n","            loss = criterion(output_ab, input_ab)\n","            losses.update(loss.item(), input_gray.size(0))\n","\n","            # Save images to file\n","            if save_images and not already_saved_images:\n","                already_saved_images = True\n","                for j in range(min(len(output_ab), 1)):  # save 1 image each epoch\n","                    save_path = {\n","                        'grayscale': '/content/drive/MyDrive/585_project_f04/outputs/gray/',\n","                        'colorized': '/content/drive/MyDrive/585_project_f04/outputs/color/',\n","                        'ground_truth': '/content/drive/MyDrive/585_project_f04/outputs/ground_truth/'\n","                    }\n","                    save_name = f'img-{i * val_loader.batch_size + j}-epoch-{epoch + 1}.jpg'\n","                    self.to_rgb(input_gray[j].cpu(), ab_input=output_ab[j].detach().cpu(), save_path=save_path, save_name=save_name)\n","\n","                    # Saving ground truth images\n","                    gt_image = torch.cat((input_gray[j].cpu(), input_ab[j].cpu()), 0).numpy()\n","                    gt_image = gt_image.transpose((1, 2, 0))\n","                    gt_image[:, :, 0:1] = gt_image[:, :, 0:1] * 100\n","                    gt_image[:, :, 1:3] = gt_image[:, :, 1:3] * 255 - 128\n","                    gt_image = lab2rgb(gt_image.astype(np.float64))\n","                    plt.imsave(arr=gt_image, fname='{}{}'.format(save_path['ground_truth'], save_name))\n","\n","            # Record time to do forward passes and save images\n","            batch_time.update(time.time() - end)\n","            end = time.time()\n","\n","            self.writer.add_scalar(\"Validation Loss\", losses.val, epoch * len(val_loader) + i)\n","\n","            if i % 2 == 0:\n","                print(f'Validate: [{i}/{len(val_loader)}]\\t'\n","                      f'Loss {losses.val:.6f} ({losses.avg:.6f})\\t')\n","\n","        print('Finished validation.')\n","        return losses.avg\n"],"metadata":{"id":"PB-DxbJF4n09","executionInfo":{"status":"ok","timestamp":1681988367106,"user_tz":240,"elapsed":3278,"user":{"displayName":"Sonu Kumar","userId":"00783746369091988188"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sqRFreVCT9LF","executionInfo":{"status":"ok","timestamp":1681988403867,"user_tz":240,"elapsed":17861,"user":{"displayName":"Sonu Kumar","userId":"00783746369091988188"}},"outputId":"7f319b1b-1dd3-48b5-8211-0eb7238b0c93"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# change depending on size of dataset, 20 if 80 in train, 1200 if 3800 in train.\n","n_val = 20\n"],"metadata":{"id":"W6im4nKZv1vh","executionInfo":{"status":"ok","timestamp":1681988406632,"user_tz":240,"elapsed":309,"user":{"displayName":"Sonu Kumar","userId":"00783746369091988188"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["epochs = 10\n","save_images = True\n","lr = 1e-3\n","weight_decay = 1e-4\n","save_model = True\n","loss = 'mse'\n","batch_size = 16"],"metadata":{"id":"D33vSvo0v5a3","executionInfo":{"status":"ok","timestamp":1681988411925,"user_tz":240,"elapsed":887,"user":{"displayName":"Sonu Kumar","userId":"00783746369091988188"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# args = argparse.Namespace(image_dir=image_dir, n_val=n_val, epochs=epochs, save_images=save_images, lr=lr, weight_decay=weight_decay, save_model=save_model, loss=loss, batch_size=batch_size)\n"],"metadata":{"id":"z3u7_DS3wC32"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = argparse.Namespace( n_val=n_val, epochs=epochs, save_images=save_images, lr=lr, weight_decay=weight_decay, save_model=save_model, loss=loss, batch_size=batch_size)\n"],"metadata":{"id":"5rpwAVH7wHhA","executionInfo":{"status":"ok","timestamp":1681988414581,"user_tz":240,"elapsed":218,"user":{"displayName":"Sonu Kumar","userId":"00783746369091988188"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"id":"2X_LU_F4R4wF","executionInfo":{"status":"ok","timestamp":1681988417488,"user_tz":240,"elapsed":777,"user":{"displayName":"Sonu Kumar","userId":"00783746369091988188"}}},"outputs":[],"source":["files = glob.glob('/content/drive/MyDrive/585_project_f04/outputs/color/*')\n","for f in files:\n","    os.remove(f)\n","files2 = glob.glob('/content/drive/MyDrive/585_project_f04/outputs/gray/*')\n","for f in files2:\n","    os.remove(f)\n","files2 = glob.glob('/content/drive/MyDrive/585_project_f04/outputs/ground_truth/*')\n","for f in files2:\n","    os.remove(f)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"UqJca87P4lyC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# model selction"],"metadata":{"id":"-wGUdw9Dw32J"}},{"cell_type":"code","source":["# run one of these each time to pick models"],"metadata":{"id":"IKe187egKUAS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# res18v1\n","model = ColorizationNet1().cuda()"],"metadata":{"id":"N44rYbpK7KfF","executionInfo":{"status":"ok","timestamp":1681988420650,"user_tz":240,"elapsed":2,"user":{"displayName":"Sonu Kumar","userId":"00783746369091988188"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# res18v2\n","model = ColorizationNet2().cuda()"],"metadata":{"id":"ZbBXEsNYKR2n","executionInfo":{"status":"ok","timestamp":1681985802216,"user_tz":240,"elapsed":342,"user":{"displayName":"Sonu Kumar","userId":"00783746369091988188"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["# res18v3\n","model = ColorizationNet3().cuda()"],"metadata":{"id":"RqIq49kkMs-q","executionInfo":{"status":"ok","timestamp":1681986352482,"user_tz":240,"elapsed":1336,"user":{"displayName":"Sonu Kumar","userId":"00783746369091988188"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["# new\n","writer = SummaryWriter()\n","if args.loss == 'mse':  # Initialize loss according to choice\n","    criterion = nn.MSELoss().cuda()\n","else:\n","    criterion = nn.L1Loss().cuda()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5, verbose=False)\n","\n","# Training\n","train_transforms = get_colorize_data_transforms(is_train=True)\n","train_imagefolder = ColorizeData('/content/drive/MyDrive/585_project_f04/images_vs/train', transform=train_transforms, is_train=True)\n","train_loader = torch.utils.data.DataLoader(train_imagefolder, batch_size=args.batch_size, shuffle=True)\n","\n","# Validation\n","val_transforms = get_colorize_data_transforms(is_train=False)\n","val_imagefolder = ColorizeData('/content/drive/MyDrive/585_project_f04/images_vs/val', transform=val_transforms, is_train=False)\n","val_loader = torch.utils.data.DataLoader(val_imagefolder, batch_size=args.batch_size, shuffle=False)\n","\n","print(\"Image preprocessing completed!\")\n","\n","# Train model\n","trainer = Trainer(writer=writer)\n","for epoch in range(args.epochs):\n","    # Train for one epoch, then validate\n","    trainer.train(train_loader, epoch, model, criterion, optimizer, scheduler)\n","    scheduler.step()\n","    with torch.no_grad():\n","        trainer.validate(val_loader, epoch, args.save_images, model, criterion)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"RXbHOhmyzFcg","executionInfo":{"status":"ok","timestamp":1681988497384,"user_tz":240,"elapsed":75424,"user":{"displayName":"Sonu Kumar","userId":"00783746369091988188"}},"outputId":"67ca69b8-79c3-40be-8ace-6caf70efc643"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Image preprocessing completed!\n","Starting training epoch 1\n","Epoch: [1][0/5]\tLoss 0.276008 (0.276008)\t\n","Epoch: [1][2/5]\tLoss 0.123127 (0.201784)\t\n","Epoch: [1][4/5]\tLoss 0.048558 (0.145692)\t\n","Finished training epoch 1\n","Validate: [0/2]\tLoss 0.197326 (0.197326)\t\n","Finished validation.\n","Starting training epoch 2\n","Epoch: [2][0/5]\tLoss 0.022842 (0.022842)\t\n","Epoch: [2][2/5]\tLoss 0.011467 (0.015421)\t\n","Epoch: [2][4/5]\tLoss 0.014301 (0.015612)\t\n","Finished training epoch 2\n","Validate: [0/2]\tLoss 0.063808 (0.063808)\t\n","Finished validation.\n","Starting training epoch 3\n","Epoch: [3][0/5]\tLoss 0.016651 (0.016651)\t\n","Epoch: [3][2/5]\tLoss 0.022872 (0.021561)\t\n","Epoch: [3][4/5]\tLoss 0.015829 (0.019667)\t\n","Finished training epoch 3\n","Validate: [0/2]\tLoss 0.040937 (0.040937)\t\n","Finished validation.\n","Starting training epoch 4\n","Epoch: [4][0/5]\tLoss 0.010962 (0.010962)\t\n","Epoch: [4][2/5]\tLoss 0.008174 (0.011111)\t\n","Epoch: [4][4/5]\tLoss 0.006730 (0.009870)\t\n","Finished training epoch 4\n","Validate: [0/2]\tLoss 0.017059 (0.017059)\t\n","Finished validation.\n","Starting training epoch 5\n","Epoch: [5][0/5]\tLoss 0.006235 (0.006235)\t\n","Epoch: [5][2/5]\tLoss 0.007257 (0.007893)\t\n","Epoch: [5][4/5]\tLoss 0.006952 (0.007516)\t\n","Finished training epoch 5\n","Validate: [0/2]\tLoss 0.009039 (0.009039)\t\n","Finished validation.\n","Starting training epoch 6\n","Epoch: [6][0/5]\tLoss 0.006476 (0.006476)\t\n","Epoch: [6][2/5]\tLoss 0.007801 (0.007054)\t\n","Epoch: [6][4/5]\tLoss 0.007211 (0.007003)\t\n","Finished training epoch 6\n","Validate: [0/2]\tLoss 0.005810 (0.005810)\t\n","Finished validation.\n","Starting training epoch 7\n","Epoch: [7][0/5]\tLoss 0.007631 (0.007631)\t\n","Epoch: [7][2/5]\tLoss 0.008239 (0.006839)\t\n","Epoch: [7][4/5]\tLoss 0.007021 (0.006311)\t\n","Finished training epoch 7\n","Validate: [0/2]\tLoss 0.004021 (0.004021)\t\n","Finished validation.\n","Starting training epoch 8\n","Epoch: [8][0/5]\tLoss 0.006183 (0.006183)\t\n","Epoch: [8][2/5]\tLoss 0.004394 (0.005064)\t\n","Epoch: [8][4/5]\tLoss 0.008576 (0.005945)\t\n","Finished training epoch 8\n","Validate: [0/2]\tLoss 0.003498 (0.003498)\t\n","Finished validation.\n","Starting training epoch 9\n","Epoch: [9][0/5]\tLoss 0.005226 (0.005226)\t\n","Epoch: [9][2/5]\tLoss 0.006385 (0.005533)\t\n","Epoch: [9][4/5]\tLoss 0.005402 (0.005714)\t\n","Finished training epoch 9\n","Validate: [0/2]\tLoss 0.003199 (0.003199)\t\n","Finished validation.\n","Starting training epoch 10\n","Epoch: [10][0/5]\tLoss 0.005770 (0.005770)\t\n","Epoch: [10][2/5]\tLoss 0.004600 (0.004931)\t\n","Epoch: [10][4/5]\tLoss 0.006232 (0.005512)\t\n","Finished training epoch 10\n","Validate: [0/2]\tLoss 0.003257 (0.003257)\t\n","Finished validation.\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"code","source":["if args.save_model:  # Saving final model\n","    torch.save(model, '/content/drive/MyDrive/585_project_f04/Models/saved_model_2.pth')"],"metadata":{"id":"0qSoaw69uG1n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-XKqhU08R4wG"},"source":["# inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7k0vEflxR4wG"},"outputs":[],"source":["import argparse\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","from skimage.color import lab2rgb, rgb2gray\n","import numpy as np\n","import os\n","import cv2\n","\n","def to_rgb(grayscale_input, ab_input):\n","  # Show/save rgb image from grayscale and ab channels\n","  plt.clf() # clear matplotlib \n","  color_image = torch.cat((grayscale_input, ab_input), 0).numpy() # combine channels\n","  color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n","  color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n","  color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128   \n","  color_image = lab2rgb(color_image.astype(np.float64))\n","  plt.imsave(arr=color_image, fname='/content/drive/MyDrive/585_project_f04/inference/inference_output.jpg')\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"orig_nbformat":4,"colab":{"provenance":[{"file_id":"1j-_2uyzOEPobiXdVwlCnhBtiu6jWuSUk","timestamp":1681977068665},{"file_id":"1jRkB0VzvZJxI63hVva8zGlcDMj80jAZ_","timestamp":1681966163943}]},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}